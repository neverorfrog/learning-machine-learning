{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression ([Book](https://d2l.ai/chapter_linear-regression/linear-regression.html#linear-regression))\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "- Target value y (better said its conditional mean $ E[Y | X = x]$ ) is a linear combination of features **x** of sample x\n",
    "- Observation noise, which causes deviation of y from its expected value, follows a gaussian\n",
    "- Notation: superscript for ith sample, subscript for ith feature of a sample\n",
    "\n",
    "## Loss Function (or how to measure the performance of our model)\n",
    "\n",
    "- It quantifies the distance between real and predicted values\n",
    "- The loss function over the entire model, we call it L, is the average of losses over every single example\n",
    "- Our goal is to find the minimum of L\n",
    "- Detail: in case of linearity, L is a function of the weights $w$ and the bias $b$\n",
    "\n",
    "## Gradient descent (or how to iteratively reducing the error)\n",
    "\n",
    "The goal is to find the optimal $\\hat w$ and $\\hat b$ and the steps of the algorithm are\n",
    "1. We select a batch of training examples of dimension $B$\n",
    "2. We evaluate the gradient (over $w$ and $b$) of the loss of each example in the batch\n",
    "3. We take the mean of all gradient evaluations\n",
    "4. We update the parameters $w$ and $b$ in direction of the negative gradient with a step size $ \\eta $\n",
    "\n",
    "## Probabilistic Interpretation\n",
    "\n",
    "- SGD also obtained from considering as objective function not the loss, but the likelihood\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code example : Linear Regression On Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([5, 3]) \n",
      "y shape: torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/flavio/code/main')\n",
    "from core.datamodule import Dataset\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_PARAMS = {\n",
    "    'use_weighted_sampler': False,\n",
    "}\n",
    "\n",
    "class SyntheticRegressionData(Dataset): \n",
    "    \"\"\"Synthetic data generator for linear regression.\"\"\"\n",
    "    def __init__(self, w, b, noise=0.01, num_train=900, num_test=1000, num_val=100):\n",
    "        X_train = torch.randn(num_train, len(w)) #design matrix X (of features)\n",
    "        train_noise = torch.randn(num_train, 1) * noise\n",
    "        y_train = torch.matmul(X_train, w.reshape((-1, 1))) + b + train_noise #vector of labels\n",
    "        train_data = torch.utils.data.TensorDataset(*[X_train, y_train])\n",
    "        \n",
    "        X_test = torch.randn(num_test, len(w)) #design matrix X (of features)\n",
    "        test_noise = torch.randn(num_test, 1) * noise\n",
    "        y_test = torch.matmul(X_test, w.reshape((-1, 1))) + b + test_noise #vector of labels\n",
    "        test_data = torch.utils.data.TensorDataset(*[X_test, y_test])\n",
    "        \n",
    "        X_val = torch.randn(num_val, len(w)) #design matrix X (of features)\n",
    "        val_noise = torch.randn(num_val, 1) * noise\n",
    "        y_val = torch.matmul(X_val, w.reshape((-1, 1))) + b + val_noise #vector of labels\n",
    "        val_data = torch.utils.data.TensorDataset(*[X_val, y_val])\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        super().__init__(load=False,params=DATA_PARAMS,train_data=train_data,test_data=test_data,val_data=val_data)\n",
    "       \n",
    "w = torch.rand(3) \n",
    "b = torch.rand(1) \n",
    "batch_size = 5\n",
    "dataset = SyntheticRegressionData(w=w,b=b)\n",
    "#Extract next minibatch\n",
    "X, y = next(iter(dataset.val_dataloader(batch_size)))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# class TXTDataLoader(Dataset):\n",
    "#     def __init__(self,txtfile):\n",
    "#         super().__init__()\n",
    "#         data = np.loadtxt(txtfile, delimiter=',')\n",
    "#         X = data[:,:-1]\n",
    "#         y = data[:,-1]\n",
    "#         self.X = torch.tensor(X).type(torch.float32)\n",
    "#         self.y = torch.tensor(y).type(torch.float32)\n",
    "#         self.num_train = 70\n",
    "#         self.num_test = 30\n",
    "#         self.batch_size = 30\n",
    "# data = DataLoader(\"data/lin_reg.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 SCORE: 0.000 LOSS: 0.351\n",
      "EPOCH 2 SCORE: 0.000 LOSS: 0.304\n",
      "EPOCH 3 SCORE: 0.000 LOSS: 0.263\n",
      "EPOCH 4 SCORE: 0.000 LOSS: 0.227\n",
      "EPOCH 5 SCORE: 0.000 LOSS: 0.198\n",
      "EPOCH 6 SCORE: 0.000 LOSS: 0.173\n",
      "EPOCH 7 SCORE: 0.000 LOSS: 0.151\n",
      "EPOCH 8 SCORE: 0.000 LOSS: 0.134\n",
      "EPOCH 9 SCORE: 0.000 LOSS: 0.119\n",
      "EPOCH 10 SCORE: 0.000 LOSS: 0.107\n",
      "EPOCH 11 SCORE: 0.000 LOSS: 0.098\n",
      "EPOCH 12 SCORE: 0.000 LOSS: 0.091\n",
      "EPOCH 13 SCORE: 0.000 LOSS: 0.086\n",
      "EPOCH 14 SCORE: 0.000 LOSS: 0.081\n",
      "EPOCH 15 SCORE: 0.000 LOSS: 0.079\n",
      "w = tensor([[0.2979],\n",
      "        [0.3044],\n",
      "        [0.5113]], requires_grad=True)\n",
      "b = tensor([[0.4514]], requires_grad=True)\n",
      "error in estimating w: tensor([0.0424, 0.0069, 0.4195], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from core.trainer import Trainer\n",
    "from core.utils import *\n",
    "from core.model import Model\n",
    "\n",
    "class LinearRegressionScratch(Model): \n",
    "    \"\"\"The linear regression model implemented from scratch.\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.save_parameters()\n",
    "        self.w = torch.zeros((input_dim, 1), requires_grad= True) \n",
    "        self.b = torch.zeros((1,1), requires_grad= True)\n",
    "\n",
    "    #That's basically all our model amounts to when computing a label\n",
    "    def forward(self, X):\n",
    "        return torch.matmul(X, self.w) + self.b\n",
    "    \n",
    "    def parameters(self):\n",
    "        return (self.w, self.b)\n",
    "\n",
    "# The loss function is computed over all the samples in considered minibatch\n",
    "def loss(y_hat, y) -> nn.Module:\n",
    "    return torch.mean(torch.pow(y_hat - y, 2) / 2)\n",
    "\n",
    "class LinearRegressionTrainer(Trainer):\n",
    "    def train_step(self, model, batch) -> None:\n",
    "        #Forward Propagation\n",
    "        X = torch.tensor(*batch[:-1]) #features\n",
    "        y_hat = model(X) #extraction of X and forward propagation\n",
    "        y = batch[-1] #labels\n",
    "        loss = self.loss_function(y_hat, y)\n",
    "        \n",
    "        #Backward Propagation\n",
    "        error = (y_hat - y)\n",
    "        n = len(model.w) #number of features\n",
    "        m = self.batch_size #number of examples\n",
    "        \n",
    "        dj_db = (1 / m) * error.sum()\n",
    "        \n",
    "        dj_dw = torch.zeros((n,1))\n",
    "        for k in range(n):\n",
    "            dj_dw[k] = (1 / m) * ((error * X[:,k]).sum()).item()\n",
    "        \n",
    "        self.w = model.w - self.lr * dj_dw\n",
    "        self.b = model.b - self.lr * dj_db\n",
    "        return loss\n",
    "\n",
    "TRAIN_PARAMS = {\n",
    "    'max_epochs': 15,\n",
    "    'learning_rate': 0.005,\n",
    "    'batch_size': 128,\n",
    "    'patience': 5,\n",
    "    'metrics': 'accuracy',\n",
    "    'optim_function': torch.optim.Adam,\n",
    "    'weight_decay': 0.001,\n",
    "    'loss_function': loss\n",
    "}\n",
    "        \n",
    "trainer = LinearRegressionTrainer(TRAIN_PARAMS)\n",
    "model = LinearRegressionScratch(3)\n",
    "trainer.fit(model,dataset)\n",
    "w,b = model.parameters()\n",
    "print(f\"w = {w}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f'error in estimating w: {dataset.w - model.w.reshape(dataset.w.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(Model):\n",
    "    \"\"\"The linear regression model implemented with high-level APIs.\"\"\"\n",
    "    def __init__(self, input_dim, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Linear(input_dim, 1, bias = True)\n",
    "        self.net.weight.data.normal_(0, 0.01)\n",
    "        self.net.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        fn = nn.MSELoss()\n",
    "        return fn(y_hat, y)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), self.lr)\n",
    "\n",
    "    def get_w_b(self):\n",
    "        return (self.net.weight.data, self.net.bias.data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate design matrix and labels with a priori defined weights and bias\n",
    "# data = SyntheticRegressionData(w, b = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
