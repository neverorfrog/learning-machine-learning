{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does it mean to learn?\n",
    "\n",
    "\n",
    "## Intuitively\n",
    "\n",
    "### Generalization\n",
    "\n",
    "Take for example a student which has to answer some questions on a test. Generalizing means that, if he saw some specific questions with the associated (corrected) answers, he is able to treat correctly new, unseen (related) questions.\n",
    "\n",
    "But what does this mean in terms of Machine Learning?\n",
    "\n",
    "### Induction\n",
    "\n",
    "Take a recommender system. This system has to predict how much (on a scale) a student likes a course. The induction framework does this:\n",
    "1. It looks at previous years' **examples** (course student pairs) taken from the so called **training set** and **induces** a function f that will map new examples to a **predicted** rating\n",
    "2. It **evaluates** the induced function against the **test set**\n",
    "\n",
    "## Formalizing\n",
    "\n",
    "- The sample set is X $ \\rightarrow $ we extract every sample x from this set\n",
    "- The evaluation/objective function is f $ \\rightarrow $ goal to reach, constructed over known data\n",
    "- The learned function is $ \\hat f $ $ \\rightarrow $ has to emulate the unknown function as best as possible\n",
    "- The loss function is $ l(f,\\hat f) $ $ \\rightarrow $ quantifies the error \n",
    "- The dataset D $ \\rightarrow $ is extracted from data distribution *D*\n",
    "- The expected error $ \\epsilon = \\sum [D(x,f(x))*l(f(x),\\hat f(x))] $ $ \\rightarrow $ expected value of l over distribution *D*\n",
    "\n",
    "![induction framework](images/b9b18ab2b16b29486abb53e8c5258f47be06603bd42e1794851a237e531a5efe.png)\n",
    "\n",
    "Step 1 is contained in the red box, aka the learning algorithm. It is executed as a cycle on all the training samples. In the figure below we zoom in. After having trained $ \\hat f(x) $, we test it, hoping that $ \\hat f(x) \\approx f(x) \\space \\forall x \\in X / D $. For that reason, the test set can never be taken from the training set. It is fundamental to avoid overfitting\n",
    "\n",
    "![Learning algorithm](images/05b31c1af1208989ecae2cc06d88a187151eaa00f02caef5dc16af2b94e15b5b.png)\n",
    "\n",
    "\n",
    "### In one sentence\n",
    "Machine learning is the process of computing a function that has as input samples from and unknown distribution and as output a value with low expected error (over that distirbution) wrt a loss function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limits of learning\n",
    "\n",
    "- Inductive bias\n",
    "- Noise\n",
    "- Overfitting and underfitting\n",
    "- No one gives us the data distribution *D*, but only a sampling of it. If we would have *D*, it would suffice to output for each sample input, the sample output for which the probability of them being coupled is maximised (Bayes optimal)\n",
    "  - This means also we can only approximate the true error with the sampling error **FORMULA TODO**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categories of models\n",
    "\n",
    "We usually differentiate every induction framework for three aspects\n",
    "   1. How we represent the function (hypothesis space)\n",
    "   2. How we evaluate the function (objective function)\n",
    "   3. How we optimize the function (loss function)\n",
    "\n",
    "We can distinguish between two big categories of machine learning models \n",
    "- Parametric models, that have a fixed number of parameters. With this approach, the learning algorithm has as output of the optimization process the optimal parameters $ \\omega ^* $, with which we then predict on test samples. After that, training data is not used anymore.\n",
    "- Non parametric models, whose number of parameters can grow with the dimension |D| of the input data. With this approach the trained model makes predictions still using the available data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
