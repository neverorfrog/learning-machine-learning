{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Multilayer Perceptrons are bad for images (or audio)\n",
    "- Dimensionality \n",
    "    - Images can be very big and this would cause an explosion of the number of parameters\n",
    "- Training\n",
    "    - Local minima\n",
    "    - Exploding and vanishing gradients due to dense layers\n",
    "\n",
    "How can we define a different kind of layer? \n",
    "- We need to exploit spatial information (nearby pixels are probably related)\n",
    "- Fully connected layers flatten the image, losing ordering of pixels\n",
    "\n",
    "# Convolution (Discrete)\n",
    "\n",
    "- 1D Convolution\n",
    "    - A linear operation where two sequences of numbers output a third sequence\n",
    "    - Ingredients\n",
    "        - Input sequence x, kernel w, output sequence z\n",
    "        - Input and output of length m, kernel of length n\n",
    "    - $(x * w)_n[p] = \\sum_i x_{p+i} \\cdot w_{n-i}$\n",
    "        - The p-th element of the convolution is a dot product\n",
    "    - Intuitively, flipping the convolver *b* and sliding it from left to right along convolvee *a*\n",
    "\n",
    "- 2D Convolution\n",
    "    - What is the result? What are the inputs?\n",
    "    - What is the operation?\n",
    "\n",
    "- In any case, the same set of weights is used everywhere\n",
    "\n",
    "# Why are convolutions useful\n",
    "\n",
    "- Images (and other signal type data) have some properties that bias the learning process in some way\n",
    "    - Translation invariance: an object remains the same if it is in different regions\n",
    "    - Locality: an object must be recognized independently of what is around\n",
    "- Depending on the task there might be other properties that bias the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layer\n",
    "- Input : image of dimension (c,w,h) = (channels, width, height)\n",
    "- Parameters : filters (learnable) usually much smaller than the image itself\n",
    "- Output : a 3D map of features of dimension (c,w,h) where c is the number of channels (features) we want to learn, while (w,h) is the dimension of the image after the convolution operation\n",
    "\n",
    "### Local receptive field\n",
    "- Every hidden unit looks at a specific region of the data\n",
    "- For example, in a 28x28 image, the LRF could be a 5x5 region\n",
    "- **Stride** length is how much we shift the LRF from one unit to the other\n",
    "- On each layer we iterate over alle the LRF\n",
    "\n",
    "### Shared weights and biases\n",
    "- The same set of wandb is associated to every LRF in the same layer\n",
    "- This means every hidden unit detects the same feature in each LRF\n",
    "- **Feature map** is what lies between the input layer and the hidden layer, the shared wandb are called **kernel**\n",
    "\n",
    "### Pooling\n",
    "- Layer after the convolutional to condense what comes out of it"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
