{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Process (evaluation and practical problems)\n",
    "\n",
    "- Overfitting\n",
    "    - Provide a formal and general definition of overfitting, without referring to any particular model\n",
    "        - Premise: we have two learning algorithms, a test set T and a training set\n",
    "        S. Moreover we have an estimator for the error between the learned\n",
    "        function and the true function. Overfitting describes a situation in which\n",
    "        $error_1(T) > error_2(T)$ and $error_1(S) < error_2(S)$. Specifically,\n",
    "        algorithm 1 is said to be overfitting on training data, because it behaves\n",
    "        better on training data, but worse on test data than algorithm 2\n",
    "        - More intuitively, it indicates the fact the hypothesis space is too expressive and too powerful, thus capable of catching too many features of the training data, while potentially ignoring features of data never seen before. \n",
    "        - Overfitting hinders generalization\n",
    "    - Show two examples of overfitting in two distinct models\n",
    "        - Linear Regression\n",
    "        - Decision Trees\n",
    "    - For one of the models above, explain how the problem can be mitigated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic interpretation of Learning\n",
    "\n",
    "- Maximum likelihood hypothesis\n",
    "    - Provide a formal deﬁnition\n",
    "        - Premise: we are in the context of parametric models, where the goal is to learn a function $f$ with a model $\\phi = g[x,\\theta]$. The input is $x$, $\\theta$ are the parameters of the model and  $\\phi$ are the parameters of the probability distribution $P(y|x;\\phi)$. This probability distribution tells us how likely a value y is, given the input x, based on the parameters $\\phi$. This probability distribution is the likelihood function. Given this information, the maximum likelihood hypothesis states that maximising this likelihood function \n",
    "\n",
    "- Maximum a Posteriori hypothesis\n",
    "    - Provide a formal definition\n",
    "        - $h = \\argmax_h P(h|D)$\n",
    "    - Provide a formal definition of Optimal Bayes Classifier\n",
    "        - "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
