{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models\n",
    "- Dataset is now without labels\n",
    "- How do represent data?\n",
    "    - We can represent the dataset by a mixture of gaussians\n",
    "    - $P(x) = \\sum_k \\pi_k N(x; \\mu_k, \\Sigma_k)$\n",
    "- Given unlabeled data, how can we reconstruct $P(x)$, namely estimate the parameters of the $k$ gaussians?\n",
    "\n",
    "## K-Means Algorithm\n",
    "- Estimate the means of the gaussians\n",
    "- Iterative algorithm\n",
    "    - For every data point take the nearest centroid\n",
    "    - Move the centroids according to the \"registered\" data points by taking the average distance\n",
    "    - Repeat until convergence\n",
    "    - Converges always but depends heavily on initialization\n",
    "- Comments  \n",
    "    - Fixed covariance with euclidean distance (so only circle shaped stuff is recognized)\n",
    "    - Very sensitive to outliers\n",
    "\n",
    "## Latent Variables\n",
    "\n",
    "- Definition\n",
    "    - Each sample $x$ has a $k$-dimensional variable $z$ where $z_k=1$ if the $k$-th gaussian generated that sample\n",
    "    - So $P(z_k=1|x) = \\pi_k \\rightarrow$ the bigger the gaussian, the bigger $\\pi_k$\n",
    "\n",
    "- Usage\n",
    "    - The goal is to estimate $z$ for each data point $\\rightarrow P(z) = \\Pi_k \\pi_k ^{z_k}$\n",
    "    - In the same way $P(x|z) = \\Pi_k N(x;\\mu_k,\\Sigma_k) ^{z_k} \\rightarrow$ given z, x depends only on the gaussian\n",
    "\n",
    "- Connection to GMM\n",
    "    - We can marginalize out $z$, getting thus $P(x) = \\Sigma_z P(x|z)P(z) = \\sum_k \\pi_k N(x; \\mu_k, \\Sigma_k)$\n",
    "        - GMM can be seen as marginalization over z of $P(x,z)$\n",
    "    - We can also apply Bayes Rule, getting posterior $P(z_k=1|x) = \\frac{P(z_k)P(x|z_k=1)}{P(x)} = \\frac{\\pi_k N(x;\\mu_k)}{\\sum_k \\pi_k N(x; \\mu_k, \\Sigma_k)}$\n",
    "\n",
    "## Expectation Maximization\n",
    "- The goal is to be able to express $P(x)$ as function of the parameters $\\pi, \\mu, \\Sigma$ for each datapoint\n",
    "- ML principle: $argmax P(x|\\pi, \\mu, \\Sigma)$ we find expressions of these parameters in function of $x$ and the posterior $P(z_k=1|x)$\n",
    "- Since we don't know neither the parameters, nor the posterior, we iterate after initializing the parameters\n",
    "    - **Expectation Step**: compute posterior from estimated parameters (sort of assigning centroid to datapoint)\n",
    "    - **Maximization Step**: compute parameters from posterior and dataset by maximising the likelihood (sort of realibrating centroid position and covariance)\n",
    "- Generalization of K-Means\n",
    "    - We compute also the posterior probability for each datapoint and not just a one hot encoded assignment\n",
    "    - We generalize more by using also the covariance \n",
    "- Generalizable to any distribution\n",
    "    - $\\theta$ can be the parameters of the model and some latent variables to denote a datasample\n",
    "    - We may hop from parameters to latent variables in an iterative scheme until convergence (maximising log-likelihood)\n",
    "- Comments\n",
    "    - Outliers don't create problems in correctness, but slow down the convergence\n",
    "    - Initialization may cause to find only a local maximum"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
