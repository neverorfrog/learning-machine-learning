{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression ([Book](https://d2l.ai/chapter_linear-regression/linear-regression.html#linear-regression))\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "- Target value y (better said its conditional mean $ E[Y | X = x]$ ) is a linear combination of features **x** of sample x\n",
    "- Observation noise, which causes deviation of y from its expected value, follows a gaussian\n",
    "- Notation: superscript for ith sample, subscript for ith feature of a sample\n",
    "\n",
    "## Loss Function (or how to measure the performance of our model)\n",
    "\n",
    "- It quantifies the distance between real and predicted values\n",
    "- The loss function over the entire model, we call it L, is the average of losses over every single example\n",
    "- Our goal is to find the minimum of L\n",
    "- Detail: in case of linearity, L is a function of the weights $w$ and the bias $b$\n",
    "\n",
    "## Gradient descent (or how to iteratively reducing the error)\n",
    "\n",
    "The goal is to find the optimal $\\hat w$ and $\\hat b$ and the steps of the algorithm are\n",
    "1. We select a batch of training examples of dimension $B$\n",
    "2. We evaluate the gradient (over $w$ and $b$) of the loss of each example in the batch\n",
    "3. We take the mean of all gradient evaluations\n",
    "4. We update the parameters $w$ and $b$ in direction of the negative gradient with a step size $ \\eta $\n",
    "\n",
    "## Probabilistic Interpretation\n",
    "\n",
    "- SGD also obtained from considering as objective function not the loss, but the likelihood\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code example : Linear Regression On Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([5, 3]) \n",
      "y shape: torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/flavio/code/machine-deep-learning')\n",
    "from core.datamodule import Dataset\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_PARAMS = {\n",
    "    'use_weighted_sampler': False,\n",
    "}\n",
    "\n",
    "class SyntheticRegressionData(Dataset): \n",
    "    \"\"\"Synthetic data generator for linear regression.\"\"\"\n",
    "    def __init__(self, w, b, noise=0.01, num_train=900, num_test=1000, num_val=100):\n",
    "        X_train = torch.randn(num_train, len(w)) #design matrix X (of features)\n",
    "        train_noise = torch.randn(num_train, 1) * noise\n",
    "        y_train = torch.matmul(X_train, w.reshape((-1, 1))) + b + train_noise #vector of labels\n",
    "        train_data = torch.utils.data.TensorDataset(*[X_train, y_train])\n",
    "        \n",
    "        X_test = torch.randn(num_test, len(w)) #design matrix X (of features)\n",
    "        test_noise = torch.randn(num_test, 1) * noise\n",
    "        y_test = torch.matmul(X_test, w.reshape((-1, 1))) + b + test_noise #vector of labels\n",
    "        test_data = torch.utils.data.TensorDataset(*[X_test, y_test])\n",
    "        \n",
    "        X_val = torch.randn(num_val, len(w)) #design matrix X (of features)\n",
    "        val_noise = torch.randn(num_val, 1) * noise\n",
    "        y_val = torch.matmul(X_val, w.reshape((-1, 1))) + b + val_noise #vector of labels\n",
    "        val_data = torch.utils.data.TensorDataset(*[X_val, y_val])\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        super().__init__(load=False,params=DATA_PARAMS,train_data=train_data,test_data=test_data,val_data=val_data)\n",
    "       \n",
    "w = torch.rand(3) \n",
    "b = torch.rand(1) \n",
    "batch_size = 5\n",
    "dataset = SyntheticRegressionData(w=w,b=b)\n",
    "#Extract next minibatch\n",
    "X, y = next(iter(dataset.val_dataloader(batch_size)))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 LOSS: 0.226\n",
      "EPOCH 2 LOSS: 0.166\n",
      "EPOCH 3 LOSS: 0.104\n",
      "EPOCH 4 LOSS: 0.096\n",
      "EPOCH 5 LOSS: 0.086\n",
      "EPOCH 6 LOSS: 0.072\n",
      "EPOCH 7 LOSS: 0.087\n",
      "EPOCH 8 LOSS: 0.084\n",
      "EPOCH 9 LOSS: 0.081\n",
      "EPOCH 10 LOSS: 0.106\n",
      "EPOCH 11 LOSS: 0.129\n",
      "EPOCH 12 LOSS: 0.108\n",
      "EPOCH 13 LOSS: 0.131\n",
      "EPOCH 14 LOSS: 0.130\n",
      "EPOCH 15 LOSS: 0.128\n",
      "w = tensor([[0.7507],\n",
      "        [0.2550],\n",
      "        [0.8899]], grad_fn=<SubBackward0>)\n",
      "b = tensor([[0.5027]], grad_fn=<SubBackward0>)\n",
      "error in estimating w: tensor([-0.0252,  0.1276,  0.0032], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from core.trainer import Trainer\n",
    "from core.utils import *\n",
    "from core.model import Model\n",
    "\n",
    "lr = 0.005 # since the model is implemented from scratch, the learning rate is needed here\n",
    "\n",
    "class LinearRegressionScratch(Model): \n",
    "    \"\"\"The linear regression model implemented from scratch.\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.save_parameters()\n",
    "        self.w = torch.zeros((input_dim, 1), requires_grad= True) \n",
    "        self.b = torch.zeros((1,1), requires_grad= True)\n",
    "\n",
    "    #That's basically all our model amounts to when computing a label\n",
    "    def forward(self, X):\n",
    "        return torch.matmul(X, self.w) + self.b\n",
    "    \n",
    "    # The loss function is computed over all the samples in considered minibatch\n",
    "    def loss_fn(self, y_hat, y) -> nn.Module:\n",
    "        return torch.mean(torch.pow(y_hat - y, 2) / 2)\n",
    "\n",
    "    def parameters(self):\n",
    "        return (self.w, self.b)\n",
    "    \n",
    "    def train_step(self, batch) -> None:\n",
    "        #Forward Propagation\n",
    "        X = torch.tensor(*batch[:-1]) #features\n",
    "        y_hat = self(X) #extraction of X and forward propagation\n",
    "        y = batch[-1] #labels\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        \n",
    "        #Backward Propagation\n",
    "        error = (y_hat - y)\n",
    "        n = len(self.w) #number of features\n",
    "        m = len(batch[0]) #number of examples\n",
    "        \n",
    "        dj_db = (1 / m) * error.sum()\n",
    "        \n",
    "        dj_dw = torch.zeros((n,1))\n",
    "        for k in range(n):\n",
    "            dj_dw[k] = (1 / m) * ((error * X[:,k]).sum()).item()\n",
    "        \n",
    "        self.w = self.w - lr * dj_dw\n",
    "        self.b = self.b - lr * dj_db\n",
    "        return loss\n",
    "    \n",
    "    def eval_step(self,batch):\n",
    "        inputs = torch.tensor(*batch[:-1]) #one sample on each row -> X.shape = (m, d_in)\n",
    "        labels = batch[-1].type(torch.long) # labels -> shape = (m)\n",
    "        logits = self(inputs)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return loss\n",
    "    \n",
    "class LinearRegressionScratchTrainer(Trainer):\n",
    "    def fit(self, model, data):\n",
    "        #stuff for dataset\n",
    "        self.batch_size = self.params['batch_size']\n",
    "        train_dataloader = data.train_dataloader(self.batch_size)\n",
    "        val_dataloader = data.val_dataloader(self.batch_size)\n",
    "        max_epochs = self.params['max_epochs']   \n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            self.fit_epoch(epoch, model,train_dataloader, val_dataloader)  \n",
    "            \n",
    "    def fit_epoch(self, epoch, model, train_dataloader, val_dataloader):\n",
    "        for batch in train_dataloader:\n",
    "            # Forward propagation\n",
    "            loss = model.train_step(batch)\n",
    "            \n",
    "        epoch_loss = 0           \n",
    "        for batch in val_dataloader:\n",
    "            #Forward propagation\n",
    "            loss = model.eval_step(batch)\n",
    "            #Logging\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        epoch_loss /= len(val_dataloader) \n",
    "        print(f\"EPOCH {epoch} LOSS: {epoch_loss:.3f}\")\n",
    "        \n",
    "\n",
    "TRAIN_PARAMS = {\n",
    "    'max_epochs': 15,\n",
    "    'batch_size': 32\n",
    "}\n",
    "        \n",
    "trainer = LinearRegressionScratchTrainer(TRAIN_PARAMS)\n",
    "model = LinearRegressionScratch(3)\n",
    "trainer.fit(model,dataset)\n",
    "w,b = model.parameters()\n",
    "print(f\"w = {w}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f'error in estimating w: {dataset.w - model.w.reshape(dataset.w.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 LOSS: 0.956\n",
      "EPOCH 2 LOSS: 0.468\n",
      "EPOCH 3 LOSS: 0.282\n",
      "EPOCH 4 LOSS: 0.168\n",
      "EPOCH 5 LOSS: 0.092\n",
      "EPOCH 6 LOSS: 0.041\n",
      "EPOCH 7 LOSS: 0.023\n",
      "EPOCH 8 LOSS: 0.016\n",
      "EPOCH 9 LOSS: 0.008\n",
      "EPOCH 10 LOSS: 0.004\n",
      "w = Parameter containing:\n",
      "tensor([[0.6987, 0.3571, 0.8458]], requires_grad=True)\n",
      "b = Parameter containing:\n",
      "tensor([0.5377], requires_grad=True)\n",
      "error in estimating w: tensor([0.0269, 0.0256, 0.0473], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class LinearRegression(Model):\n",
    "    \"\"\"The linear regression model implemented with high-level APIs.\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(input_dim, 1, bias = True)\n",
    "        self.net.weight.data.normal_(0, 0.01)\n",
    "        self.net.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "    def loss_fn(self, y_hat, y):\n",
    "        fn = nn.MSELoss()\n",
    "        return fn(y_hat, y)\n",
    "    \n",
    "    def train_step(self, batch) -> None:\n",
    "        inputs = torch.tensor(*batch[:-1]) # one sample on each row -> X.shape = (m, d_in)\n",
    "        logits = self(inputs) # inference\n",
    "        labels = batch[-1] # labels -> shape = (m)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return loss\n",
    "    \n",
    "    def eval_step(self,batch):\n",
    "        with torch.no_grad():\n",
    "            loss = self.train_step(batch)\n",
    "        return loss\n",
    "    \n",
    "class LinearRegressionTrainer(Trainer):\n",
    "    def fit(self, model, data):\n",
    "        #stuff for dataset\n",
    "        self.batch_size = self.params['batch_size']\n",
    "        train_dataloader = data.train_dataloader(self.batch_size)\n",
    "        val_dataloader = data.val_dataloader(self.batch_size)\n",
    "        self.lr = self.params['learning_rate']\n",
    "        max_epochs = self.params['max_epochs']  \n",
    "        optim = self.params['optim_function'](model.parameters(), lr=self.lr, weight_decay=self.params['weight_decay']) \n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            self.fit_epoch(epoch, model,optim,train_dataloader, val_dataloader)  \n",
    "            \n",
    "    def fit_epoch(self, epoch, model, optim, train_dataloader, val_dataloader):\n",
    "        \n",
    "        # Training Mode\n",
    "        model.train() \n",
    "        for batch in train_dataloader:\n",
    "            # Forward propagation\n",
    "            loss = model.train_step(batch)\n",
    "            # Backward Propagation\n",
    "            optim.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                loss.backward() #here we calculate the chained derivatives (every parameters will have .grad changed)\n",
    "                optim.step() \n",
    "                \n",
    "        # Evaluation Mode        \n",
    "        model.eval()    \n",
    "        epoch_loss = 0           \n",
    "        for batch in val_dataloader:\n",
    "            #Forward propagation\n",
    "            loss = model.eval_step(batch)\n",
    "            #Logging\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        epoch_loss /= len(val_dataloader) \n",
    "        print(f\"EPOCH {epoch} LOSS: {epoch_loss:.3f}\")\n",
    "    \n",
    "TRAIN_PARAMS = {\n",
    "    'max_epochs': 10,\n",
    "    'learning_rate': 0.005,\n",
    "    'batch_size': 32,\n",
    "    'optim_function': torch.optim.SGD,\n",
    "    'weight_decay': 0.001\n",
    "}\n",
    "        \n",
    "trainer = LinearRegressionTrainer(TRAIN_PARAMS)\n",
    "model = LinearRegression(3)\n",
    "trainer.fit(model,dataset)\n",
    "w,b = model.parameters()\n",
    "print(f\"w = {w}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f'error in estimating w: {dataset.w - model.net.weight.reshape(dataset.w.shape)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
