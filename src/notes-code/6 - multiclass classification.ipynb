{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition (For now just one layer)\n",
    "\n",
    "- It's just like logistic regression, but the loss function changes (because the output dimension changes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox.base_models import *\n",
    "from toolbox.trainer import *\n",
    "from toolbox.datamodule import FashionMNIST\n",
    "import time\n",
    "\n",
    "data = FashionMNIST()\n",
    "(data.train)[0] #first tuple (sample, label) in the dataset\n",
    "(data.train)[0][0].shape #the image size\n",
    "X, y = next(iter(data.get_dataloader(train = True))) #first batch in form of tuple\n",
    "print(X.shape, X.dtype, y.shape, y.dtype)\n",
    "data.visualize([X,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "for X, y in data.train_dataloader():\n",
    "    continue\n",
    "f'{time.time() - tic:.2f} sec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox.base_models import *\n",
    "from toolbox.trainer import *\n",
    "from toolbox.datamodule import FashionMNIST\n",
    "\n",
    "data = FashionMNIST(batch_size=256, resize = (28,28))\n",
    "input_dim = 28*28 #number of pixels, so of weights (rows of W and columns of X), each connected to every output\n",
    "output_dim = 10 #number of classes\n",
    "model = SoftmaxRegressionScratch(input_dim, output_dim=10, lr=0.1)\n",
    "trainer = Trainer(max_epochs=20)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we fit our model to the training data, we can make some predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(data.val_dataloader()))\n",
    "predictions = model(X).argmax(axis=1)\n",
    "data.visualize([X,y])\n",
    "wrong = predictions.type(y.dtype) != y #We are though interested in the wrongly labeled examples\n",
    "X, y, predictions = X[wrong], y[wrong], predictions[wrong]\n",
    "labels = [\"y: \" + a +'\\n'+ \"y_hat: \" + b for a, b in zip(data.text_labels(y), data.text_labels(predictions))]\n",
    "data.visualize([X, y], labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox.base_models import *\n",
    "from toolbox.trainer import *\n",
    "from toolbox.datamodule import FashionMNIST\n",
    "\n",
    "data = FashionMNIST(batch_size=256)\n",
    "model = SoftmaxRegression(input_dim = 784, output_dim = 10, lr=0.1)\n",
    "trainer = Trainer(max_epochs=10)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "1. We have not used the test portion of the dataset, modify the training loop to include tracking of test loss and test accuracy.\n",
    "\n",
    "2. Add more metrics to be tracked, e.g., multi-class F1-score. The torchmetrics package has a lot of options in this sense: https://torchmetrics.readthedocs.io/en/stable/.\n",
    "\n",
    "3. Momentum is a simple technique to improve the convergence speed of gradient descent. The key idea is to update each variable using a weighted average of the current gradient, and the gradient at the previous iteration (see Section 12.6 in the book). The weighting parameter is called the momentum weight. Implement momentum in the codelab, using a weight of 0.5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
