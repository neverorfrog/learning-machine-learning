{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks (2023-2024)\n",
        "https://sites.google.com/uniroma1.it/neuralnetworks2023/\n",
        "\n",
        "This is a short notebook highlighting the use of [torch.func](https://pytorch.org/docs/stable/func.html), a PyTorch module that provides a functional interface to the framework and transformations mirroring those found in [JAX](https://jax.readthedocs.io/en/latest/) (vmap, jit, ...). The notebook wants to highlight the difference in working in OOP or in functional paradigms, and showing examples where the functional approach is simpler and more elegant due to the possibility of easily chaining functional transformations."
      ],
      "metadata": {
        "id": "qXg4agaKqm27"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQabgS5piNL_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, func"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple PyTorch model\n",
        "net = nn.Sequential(\n",
        "    nn.Linear(3, 4),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Linear(4, 5),\n",
        "    nn.Softmax(1)\n",
        ")"
      ],
      "metadata": {
        "id": "kFNpoxgniXsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters can be accessed through iterators\n",
        "net.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kT6LBTGjSrh",
        "outputId": "3450c7de-2163-4e74-838d-7f7bfa7eb613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7fe0d7305a10>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: from a functional perspective, .backward() is very strange because most of its\n",
        "# behaviour is hidden, and there are many side effects.\n",
        "x = torch.randn((10, 3))\n",
        "net(x).sum().backward()"
      ],
      "metadata": {
        "id": "cPzes7sQjpGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net[0].weight.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoR_nht-jt8q",
        "outputId": "caa53860-c390-4e20-e919-816271fd6f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.1237e-09,  3.7508e-09, -4.9393e-11],\n",
              "        [ 7.4083e-09, -1.0464e-09,  1.0326e-09],\n",
              "        [-4.3719e-10, -3.8304e-09,  6.3779e-11],\n",
              "        [ 1.2196e-09,  1.0686e-08, -1.7792e-10]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To move to a functional representation, we first extract the parameters,\n",
        "# then we convert the object instance into a pure function taking as input\n",
        "# both x and the parameters.\n",
        "w = dict(net.named_parameters())\n",
        "net_fcn = lambda w, x: func.functional_call(net, w, x)"
      ],
      "metadata": {
        "id": "dfmYfVj0kQ1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# func.grad is an operator (higher-order function) that returns a new function\n",
        "# that evaluates the gradient.\n",
        "net_grad_fcn = func.grad(lambda w, x: net_fcn(w, x).sum())"
      ],
      "metadata": {
        "id": "C1wRmfPVkyYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this approach, gradients are returned directly as output of the function.\n",
        "net_grad_fcn(w, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgf6QezplQVW",
        "outputId": "f30f5566-3ef1-40c4-b7be-b1a1b76c580b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.weight': tensor([[ 6.1237e-09,  3.7508e-09, -4.9393e-11],\n",
              "         [ 7.4083e-09, -1.0464e-09,  1.0326e-09],\n",
              "         [-4.3719e-10, -3.8304e-09,  6.3779e-11],\n",
              "         [ 1.2196e-09,  1.0686e-08, -1.7792e-10]], grad_fn=<TBackward0>),\n",
              " '0.bias': tensor([ 5.4171e-09,  1.0515e-08,  2.0189e-09, -5.6320e-09],\n",
              "        grad_fn=<ViewBackward0>),\n",
              " '2.weight': tensor([[1.6468e-08, 1.1425e-08, 5.0754e-09, 8.2010e-09],\n",
              "         [1.3497e-08, 7.5688e-09, 3.1503e-09, 5.0904e-09],\n",
              "         [3.4575e-08, 1.6428e-08, 6.0394e-09, 9.7587e-09],\n",
              "         [1.7959e-08, 8.6938e-09, 3.2041e-09, 5.1774e-09],\n",
              "         [1.8011e-08, 1.0379e-08, 4.3835e-09, 7.0831e-09]],\n",
              "        grad_fn=<TBackward0>),\n",
              " '2.bias': tensor([3.3357e-08, 2.4654e-08, 5.7422e-08, 2.9984e-08, 3.3398e-08],\n",
              "        grad_fn=<ViewBackward0>)}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose we add a new dimension on our input (multi-view input): we have a batch of 10\n",
        "# elements, each of which is composed of 5 vectors of dimension 3. Note that in this case,\n",
        "# Linear works as expected, but softmax is normalizing across a wrong axis.\n",
        "x = torch.randn((10, 5, 3))\n",
        "net(x)[0, 0].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsvFKnV6l68b",
        "outputId": "7feefc8d-00ff-494c-ff1b-d045b11d8b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9590, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use vmap to vectorize functions (apply them in parallel) over new axes.\n",
        "net_vect_fcn = func.vmap(net_fcn, in_dims=(None, 1), out_dims=1)"
      ],
      "metadata": {
        "id": "jO961dgbmIVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_vect_fcn(w, x)[0, 0].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdc_PCAXnW5L",
        "outputId": "59d99d0f-d5a8-46cc-d960-68ccd5a5a660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0000, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can combine vmap and grad as many times as needed.\n",
        "net_grad_grad_fcn = func.grad(lambda w, x:\n",
        "                         func.grad(lambda w, x: net_vect_fcn(w, x).sum())\n",
        "                         )"
      ],
      "metadata": {
        "id": "lnisQeBSni8U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}