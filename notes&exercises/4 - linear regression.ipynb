{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression ([Book](https://d2l.ai/chapter_linear-regression/linear-regression.html#linear-regression))\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "- Target value y (better said its conditional mean $ E[Y | X = x]$ ) is a linear combination of features **x** of sample x\n",
    "- Observation noise, which causes deviation of y from its expected value, follows a gaussian\n",
    "- Notation: superscript for ith sample, subscript for ith feature of a sample\n",
    "\n",
    "## Loss Function (or how to measure the performance of our model)\n",
    "\n",
    "- It quantifies the distance between real and predicted values\n",
    "- The loss function over the entire model, we call it L, is the average of losses over every single example\n",
    "- Our goal is to find the minimum of L\n",
    "- Detail: in case of linearity, L is a function of the weights $w$ and the bias $b$\n",
    "\n",
    "## Gradient descent (or how to iteratively reducing the error)\n",
    "\n",
    "The goal is to find the optimal $\\hat w$ and $\\hat b$ and the steps of the algorithm are\n",
    "1. We select a batch of training examples of dimension $B$\n",
    "2. We evaluate the gradient (over $w$ and $b$) of the loss of each example in the batch\n",
    "3. We take the mean of all gradient evaluations\n",
    "4. We update the parameters $w$ and $b$ in direction of the negative gradient with a step size $ \\eta $\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
