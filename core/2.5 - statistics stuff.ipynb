{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood Function\n",
    "- Represents how well the parameters represent the data\n",
    "    - Expressed as $L(\\theta ; X, y) = p(y | X ; \\theta)$\n",
    "    - The probability of observing the label y given data X and under the assumption that the model parameters are equal to θ\n",
    "    - Same as the probability mass function, but viewed as a function of the parameter θ given the observed data X and label Y \n",
    "\n",
    "\n",
    "- Logistic Regression\n",
    "    - Single data point: likelihood function is probability mass function of the Bernoulli distribution \n",
    "        - $l(\\theta;x,y) = [P(y=1|x)]^y \\times [1-P(y=1|x)]^{1-y}$\n",
    "        - Intuitively, probability of observing as outcome of a bernoulli trial label 0 or 1 when observing a data point\n",
    "    - More data points: assumption that all are iid according to Bernoulli\n",
    "        - $L(\\theta;x,y) = \\prod_i l(\\theta;x_i,y_i)$\n",
    "    - $P(y=1|x) = \\sigma(\\theta^T x)$ where $\\sigma$ is the sigmoid function\n",
    "\n",
    "- Linear Regression\n",
    "    - More data points: likelihood function is the product of iid gaussians\n",
    "        - $L(\\theta;x,y) \\propto \\prod_i exp (-(y_i - \\theta^T x_i)^2) $\n",
    "\n",
    "# Maximum Likelihood Estimation\n",
    "\n",
    "- Intuitively:\n",
    "    - Estimate the parameters ($\\theta$) of a function (our classifier), maximising the LIKELIHOOD function\n",
    "    - This means we have to choose some $\\theta$ that make the available data as highly probable as possible\n",
    "- Mathematically:\n",
    "    - Usually take log of L, obtaining a sum of logs\n",
    "    - Minimize it's negation $\\rightarrow$ Obtaining **negative log likelihood**\n",
    "    - That can be achieved either in closed form or through some iterative optimization algorithm (SGD)\n",
    "- Examples:\n",
    "    - Logistic Regression $\\rightarrow$ Obtaining negative log likelihood (cross entropy)\n",
    "    - Linear Regression $\\rightarrow$ Obtaining least squares loss\n",
    "- Cross-Entropy:\n",
    "    - Measures dissimilarity between empirical (sample) distribution and real distribution\n",
    "    - Needs to be minimized"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
