# Probabilistic interpretation of Learning

## Maximum likelihood hypothesis

- Provide a formal deÔ¨Ånition
  - Premise: we are in the context of parametric models, where the goal is to learn a function $f$ with a model $\phi = g[x,\theta]$. The input is $x$, $\theta$ are the parameters of the model and  $\phi$ are the parameters of the probability distribution $P(y|x;\phi)$. This probability distribution tells us how likely a value y is, given the input x, based on the parameters $\phi$. This probability distribution is the likelihood function. Given this information, the maximum likelihood hypothesis states that maximising this likelihood function

## Maximum a Posteriori hypothesis

- Provide a formal definition
  - $h = \argmax_h P(h|D)$
- Provide a formal definition of Optimal Bayes Classifier
      - Ok
